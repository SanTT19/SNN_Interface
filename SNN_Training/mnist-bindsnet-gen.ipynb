{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train and Test Data for SNN\n",
    "- Bindsnet provides the required operations using Poisson Distribution to generate spike trains.\n",
    "- Test and Train data are stored as pickle files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "from bindsnet.datasets import MNIST\n",
    "from bindsnet.encoding import PoissonEncoder\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andres\\OneDrive - Singapore University of Technology and Design\\SUTD-Lab Documentation\\Projects\\SNN-Training\\Example\n"
     ]
    }
   ],
   "source": [
    "# Path to the current directory\n",
    "my_path = os.path.abspath('')\n",
    "print(my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load train and test data from MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for Poisson Encoding\n",
    "time = 20                   # Time steps\n",
    "dt = 1.0                    # Similation time step\n",
    "intensity = 256             # Intensity for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TorchvisionDatasetWrapper\n",
      "    Number of datapoints: 60000\n",
      "    Root location: c:\\Users\\Andres\\OneDrive - Singapore University of Technology and Design\\SUTD-Lab Documentation\\Projects\\SNN-Training\\Example\\data\\MNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Lambda()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data: Training.\n",
    "train_dataset = MNIST(\n",
    "    PoissonEncoder(time=time, dt=dt),\n",
    "    None,\n",
    "    my_path + \"\\data\\MNIST\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TorchvisionDatasetWrapper\n",
      "    Number of datapoints: 10000\n",
      "    Root location: c:\\Users\\Andres\\OneDrive - Singapore University of Technology and Design\\SUTD-Lab Documentation\\Projects\\SNN-Training\\Example\\data\\MNIST\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Lambda()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data: Testing.\n",
    "test_dataset = MNIST(\n",
    "    PoissonEncoder(time=time, dt=dt),\n",
    "    None,\n",
    "    my_path + \"\\data\\MNIST\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Store spike trains to use in snnTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to store all the data\n",
    "train_l=[]\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    train_l.append(train_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data\n",
    "with open(my_path + '\\data\\mnist_train.pkl', 'wb') as f:        # Open a text file\n",
    "    pickle.dump(train_l, f)                                     # Serialize the variable\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to store all the data\n",
    "test_l=[]\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_l.append(test_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data\n",
    "with open(my_path + '\\data\\mnist_test.pkl', 'wb') as f:        # Open a text file\n",
    "    pickle.dump(test_l, f)                                     # Serialize the variable\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
